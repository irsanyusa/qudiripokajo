<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="Earlier this month, Google released its long-awaited system &amp;quot;Gemini,&amp;quot; giving users access to its AI image-generation technology for the first time. While most early users agreed that the system was impressive, creating detailed images for text prompts in seconds, users soon discovered that it was difficult to get the system to generate images of white"><meta name=generator content="Hugo 0.98.0"><meta name=robots content="index,follow,noarchive"><title>Ethical AI Isn't to Blame for Google's Gemini Debacle &#183;</title><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css><!--[if lte IE 8]><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css><![endif]--><!--[if gt IE 8]><!--><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css><!--<![endif]--><!--[if lte IE 8]><link rel=stylesheet href=https://assets.cdnweb.info/hugo/blackburn/css/side-menu-old-ie.css><![endif]--><!--[if gt IE 8]><!--><link rel=stylesheet href=https://assets.cdnweb.info/hugo/blackburn/css/side-menu.css><!--<![endif]--><link rel=stylesheet href=https://assets.cdnweb.info/hugo/blackburn/css/blackburn.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Raleway&display=swap" rel=stylesheet type=text/css><script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/styles/androidstudio.min.css><script async src=//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.6.0/highlight.min.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel="shortcut icon" href=./img/favicon.ico type=image/x-icon></head><body><div id=layout><a href=#menu id=menuLink class=menu-link><span></span></a><div id=menu><a class="pure-menu-heading brand" href=./index.html>WinkVlog</a><div class=pure-menu><ul class=pure-menu-list><li class=pure-menu-item><a class=pure-menu-link href=./index.html><i class="fa fa-home fa-fw"></i>Home</a></li><li class=pure-menu-item><a class=pure-menu-link href=./post/index.html><i class="fa fa-list fa-fw"></i>Posts</a></li><li class=pure-menu-item><a class=pure-menu-link href=./sitemap.xml><i class="fa fa-user fa-fw"></i>Sitemap</a></li><li class=pure-menu-item><a class=pure-menu-link href=./index.xml><i class="fa fa-phone fa-fw"></i>RSS</a></li></ul></div><div class="pure-menu social"><ul class=pure-menu-list></ul></div><div><div class=small-print><small>&copy; 2022. All rights reserved.</small></div><div class=small-print><small>Built with&nbsp;<a href=https://gohugo.io/ target=_blank>Hugo</a></small>
<small>Theme&nbsp;<a href=https://github.com/yoshiharuyamashita/blackburn target=_blank>Blackburn</a></small></div></div></div><div id=main><div class=header><h1>Ethical AI Isn't to Blame for Google's Gemini Debacle</h1><h2>Earlier this month, Google released its long-awaited system &amp;quot;Gemini,&amp;quot; giving users access to its AI image-generation technology for the first time. While most early users agreed that the system was impressive, creating detailed images for text prompts in seconds, users soon discovered that it was difficult to get the system to generate images of white</h2></div><div class=content><div class=post-meta><div><i class="fa fa-calendar fa-fw"></i>
<time>02 Aug 2024, 00:00</time></div></div><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><span class="leading-7 float-left border-t-2 border-l-2 border-solid border-time-red text-5xl py-2 pr-0.5 pl-[0.3125rem] my-0.5 mr-2.5 font-zilla-slab">E</span>arlier this month, Google released its long-awaited system "Gemini," giving users access to its AI image-generation technology for the first time. While most early users agreed that the system was impressive, creating detailed images for text prompts in seconds, users soon discovered that it was difficult to get the system to generate images of white people, and soon viral tweets displayed head-scratching examples such as <a href=#>racially diverse Nazis</a>.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Some people faulted Gemini for being "too woke," using Gemini as the latest weapon in an escalating culture war on the importance of recognizing the effects of historical discrimination. Many said it reflected a malaise inside Google, and some <a href=#>criticized</a> the field of "AI ethics" as an embarrassment.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The idea that ethical AI work is to blame is wrong. In fact, Gemini showed Google <a href=#>wasn't correctly applying</a> the lessons of AI ethics. Where AI ethics focuses on addressing foreseeable use cases– such as historical depictions–Gemini seems to have opted for a "one size fits all" approach, resulting in an awkward mix of refreshingly diverse and cringeworthy outputs.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">I should know. I've worked on ethics in AI within technology companies for over 10 years, making me one of the most senior experts in the world on the matter (it's a young field!). I also founded and co-led Google's "Ethical AI" team, before they fired me and my co-lead following our report warning about exactly these kinds of issues for language generation. Many people criticized Google for their decision, believing it reflected systemic discrimination and a prioritization of reckless speed over well-considered strategy in AI. It is possible I strongly agree.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The Gemini debacle again laid bare Google's inexpert strategy in areas where I'm uniquely qualified to help, and which I can now help the public understand more generally. This piece will discuss some ways that AI companies can do better next time, avoiding giving the far-right unhelpful ammunition in culture wars, and ensuring that AI benefits as many people as possible in the future.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">One of the critical pieces in operationalizing ethics in AI is to articulate foreseeable use, including malicious use and misuse. This means working through questions such as Once the model we're thinking of building is deployed, how will people use it? And how can we design it to be as beneficial as possible in these contexts? This approach recognizes the central importance of "context of use" when creating AI systems. This type of foresight and contextual thinking, grounded on the interaction of society and technology, is harder for some people than others–here is where people with expertise in human-computer interaction, social science, and cognitive science are particularly skilled (speaking to the importance of interdisciplinarity in tech hiring). These roles tend not to be given as much power and influence as engineering roles, and my guess is that this was true in the case of Gemini: those most skilled at articulating foreseeable uses were not empowered, leading to a system that could not handle multiple types of appropriate use, such as the depiction of historically white groups.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Things go wrong when organizations treat all use cases as one use case, or don't model use cases at all. As such, without an ethics-grounded analysis of use cases in different contexts, AI systems may not have models "under the hood" that help to identify what the user is asking for (and whether it should be generated). For Gemini, this could involve determining whether the user is seeking imagery that's historic or diverse, and whether their request is ambiguous or malicious. We recently saw this same failure to build robust models for foreseeable use leading to <a href=#>the proliferation of AI-generated Taylor Swift pornography</a>.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">To assist, I years ago made the following chart. The task is to fill out the cells; I've filled it out today with a few examples relevant to Gemini specifically.</p><img style=margin:auto;display:block;text-align:center;max-width:100%;height:auto src=https://cdn.statically.io/img/api.time.com/wp-content/uploads/2024/02/foresight-in-AI.jpg><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The green cells (top row) are those where beneficial AI is most likely possible (not where AI will always be beneficial). The red cells (middle row) are those where harmful AI is most likely (but may also be where unforeseen beneficial innovation may occur). The rest of the cells are more likely to have mixed results – some outcomes good, some bad.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The next steps involve working through likely errors in different contexts, addressing disproportionate errors for subgroups subject to discrimination. The developers of Gemini seem to have gotten this part largely right. The team seems to have had the foresight to recognize the risk of overrepresenting white people in neutral or positive situations, which would amplify a problematic white-dominant view of the world. And so, there was likely a submodule within Gemini designed to show darker skin tones to users.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The fact that these steps are evident in Gemini, but not the steps involving foreseeable use, may be due in part to increased public awareness of bias in AI systems: a pro-white bias was an easily foreseeable PR nightmare, echoing <a href=#>the Gorilla Incident Google has become infamous for</a>, whereas the nuanced approaches to handle "context of use" was not. The net effect was a system that "missed the mark" on being inclusive of foreseeable appropriate use cases.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The high-level point is that it is possible to have technology that benefits users and minimizes harm to those most likely to be negatively affected. But you have to have people who are good at doing this included in development and deployment decisions. And these people are often disempowered (or worse) in tech. It doesn't have to be this way: We can have different paths for AI that empower the right people for what they're most qualified to help with. Where diverse perspectives are sought out, not shut down. To get there requires some rough work and ruffled feathers. We'll know we're on a good path when we start seeing tech executives that are as diverse as the images Gemini generates.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmismaKyb6%2FOpmZvcGNrfnZ%2Fjp6roaGTlrlurchmnqinl6GybrPEpqCnoV2ZsqOtwqWcaA%3D%3D</p><h4><i class="fas fa-share-alt" aria-hidden=true></i>&nbsp;Share!</h4><ul class=share-buttons><li><a href="https://www.facebook.com/sharer/sharer.php?u=%2fethical-ai-google-gemini-debacle.html" target=_blank title="Share on Facebook"><i class="fab fa-facebook" aria-hidden=true></i><span class=sr-only>Share on Facebook</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="https://twitter.com/intent/tweet?source=%2fethical-ai-google-gemini-debacle.html" target=_blank title=Tweet><i class="fab fa-twitter" aria-hidden=true></i><span class=sr-only>Tweet</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="https://plus.google.com/share?url=%2fethical-ai-google-gemini-debacle.html" target=_blank title="Share on Google+"><i class="fab fa-google-plus" aria-hidden=true></i><span class=sr-only>Share on Google+</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="http://www.tumblr.com/share?v=3&u=%2fethical-ai-google-gemini-debacle.html" target=_blank title="Post to Tumblr"><i class="fab fa-tumblr" aria-hidden=true></i><span class=sr-only>Post to Tumblr</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="http://pinterest.com/pin/create/button/?url=%2fethical-ai-google-gemini-debacle.html" target=_blank title="Pin it"><i class="fab fa-pinterest-p" aria-hidden=true></i><span class=sr-only>Pin it</span></a></li>&nbsp;&nbsp;&nbsp;<li><a href="http://www.reddit.com/submit?url=%2fethical-ai-google-gemini-debacle.html" target=_blank title="Submit to Reddit"><i class="fab fa-reddit-alien" aria-hidden=true></i><span class=sr-only>Submit to Reddit</span></a></li></ul><style>ul.share-buttons{list-style:none;padding:0}ul.share-buttons li{display:inline}ul.share-buttons .sr-only{position:absolute;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);padding:0;border:0;height:1px;width:1px;overflow:hidden}</style><div class="prev-next-post pure-g"><div class=pure-u-1-24 style=text-align:left><a href=./le-conseil-des-ministres-adopte-le-projet-de-loi-vieillissement.html><i class="fa fa-chevron-left"></i></a></div><div class=pure-u-10-24><nav class=prev><a href=./le-conseil-des-ministres-adopte-le-projet-de-loi-vieillissement.html>Le Conseil des ministres adopte le projet de loi Vieillissement</a></nav></div><div class=pure-u-2-24>&nbsp;</div><div class=pure-u-10-24><nav class=next><a href=./the-real-dirty-dancing-corbin-bleu-is-the-first-man-cat-cora-has-kissed.html>Corbin Bleu is the First Man Cat Cora Has Kissed</a></nav></div><div class=pure-u-1-24 style=text-align:right><a href=./the-real-dirty-dancing-corbin-bleu-is-the-first-man-cat-cora-has-kissed.html><i class="fa fa-chevron-right"></i></a></div></div></div></div></div><script src=https://assets.cdnweb.info/hugo/blackburn/js/ui.js></script>
<script src=https://assets.cdnweb.info/hugo/blackburn/js/menus.js></script>
<script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>